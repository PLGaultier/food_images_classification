{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EECsDsPqYTSL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwFMHI5wVFtM",
        "outputId": "6fbb08a6-22fe-492f-b542-ebf477463499"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p4ROZXbsYnFp"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------- Fonctions -----------------------------------------------#\n",
        "\n",
        "## Resize Image function to desired shape\n",
        "def resize_image(img_shape,path,path_dir):\n",
        "    for item in tqdm(os.listdir(path)):\n",
        "        if item == '.DS_Store':\n",
        "             continue\n",
        "        if os.path.isfile(path+item):\n",
        "            im = Image.open(path+item)\n",
        "            f, e = os.path.splitext(item)\n",
        "            imResize = im.resize((img_shape[0],img_shape[1])) ## Resize image\n",
        "            imResize.save(path_dir+f+'.jpg','jpeg') ## Save it to path_dir\n",
        "\n",
        "\n",
        "## Get the features of all images with pre trained model : Inception ResNet v2. Discarding last dense\n",
        "## classification layer\n",
        "def get_features_images_Inception(input_shape,model):\n",
        "  image_res_path = '/content/drive/MyDrive/Colab Notebooks/Task3/food_res/'\n",
        "  all_features = []\n",
        "  for item in tqdm(os.listdir(image_res_path)):\n",
        "    img = image.load_img(image_res_path+item, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    img = tf.keras.applications.inception_resnet_v2.preprocess_input(x)\n",
        "    x_feat = model.predict(img,steps=10000)\n",
        "    f,s = os.path.splitext(item)\n",
        "    try:\n",
        "      features = np.insert(x_feat,0,int(f))\n",
        "      all_features.append(features)\n",
        "    except ValueError:\n",
        "      continue\n",
        "  return all_features\n",
        " \n",
        "\n",
        "#building the training/testing dataset, based on ids triplets given in the txt files of the task\n",
        "def build_raw_features(all_features, file, train_or_test_bool):\n",
        "    training_data_id = pd.read_csv(file, delim_whitespace=True, header=None, names=[\"A\", \"B\", \"C\"])\n",
        "    training_data_raw = []\n",
        "\n",
        "    if train_or_test_bool:\n",
        "        for train_point in tqdm(range(len(training_data_id))):\n",
        "            triplet = training_data_id.iloc[train_point]\n",
        "            raw_point_positive = np.concatenate((all_features[triplet['A']], all_features[triplet['B']], all_features[triplet['C']]), axis = -1)\n",
        "            raw_point_negative = np.concatenate((all_features[triplet['A']], all_features[triplet['C']], all_features[triplet['B']]), axis = -1)\n",
        "\n",
        "            training_data_raw.append(raw_point_positive)\n",
        "            training_data_raw.append(raw_point_negative)\n",
        "    else:\n",
        "        for train_point in tqdm(range(len(training_data_id))):\n",
        "            triplet = training_data_id.iloc[train_point]\n",
        "            raw_point = np.concatenate((all_features[triplet['A']], all_features[triplet['B']], all_features[triplet['C']]), axis = -1)\n",
        "            training_data_raw.append(raw_point)\n",
        "\n",
        "    return np.array(training_data_raw)\n",
        "\n",
        "#constructing the labels by doubling the training dataset. Need positive as well as negatives\n",
        "def construct_labels(file):\n",
        "  triplets_read = pd.read_csv(file, delim_whitespace=True, header=None, names=[\"A\", \"B\", \"C\"])\n",
        "  #we assume here the number of feature is even\n",
        "  labels = np.ones((len(triplets_read)*2, 1))\n",
        "\n",
        "  idx_list = [idx for idx in tqdm(range(1, len(triplets_read)*2 + 1, 2))]\n",
        "\n",
        "  labels[idx_list] = 0\n",
        "  return np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------- Main -----------------------------------------------#\n",
        "\n",
        "def main():\n",
        "  ## Boolean ##\n",
        "  resize_bol = False\n",
        "  get_features_bol = False\n",
        "  build_tensor_bol = True\n",
        "  train_or_test_bol = [True,False]\n",
        "\n",
        "  ## Path ##\n",
        "  food_dir = '/content/gdrive/MyDrive/Colab Notebooks/food/'\n",
        "  food_res_dir = '/content/drive/MyDrive/Colab Notebooks/food_res/'\n",
        "  features_file = '/content/gdrive/MyDrive/Colab Notebooks/features.pckl'\n",
        "  train_file = '/content/gdrive/MyDrive/Colab Notebooks/train_triplets.txt'\n",
        "  test_file = '/content/gdrive/MyDrive/Colab Notebooks/test_triplets.txt'\n",
        "\n",
        "  ## Shape ##\n",
        "  img_shape = [299,299]\n",
        "  input_shape = (299,299,3)\n",
        "\n",
        "  ## Model ##\n",
        "  model = tf.keras.applications.InceptionResNetV2(pooling='avg',include_top=False)\n",
        "\n",
        "\n",
        "  ## Flow ##\n",
        "  if resize_bol:\n",
        "    print('Resizing image ...')\n",
        "    resize_image(img_shape,food_dir,food_res_dir)\n",
        "\n",
        "  if get_features_bol:\n",
        "    print('Computing features ...')\n",
        "    features = get_features_images_Inception(input_shape,model)\n",
        "    with open(features_file, 'wb') as f:\n",
        "      pickle.dump(features, f)\n",
        "  else:\n",
        "    print('Loading features ...')\n",
        "    with open(features_file, 'rb') as f:\n",
        "      features = pickle.load(f)\n",
        "\n",
        "  ## Sorting the features ascending ## \n",
        "  features = np.array(features)\n",
        "  features_sort = features[features[:, 0].argsort()]\n",
        "  features_sort = np.delete(features_sort, 0, axis=1)\n",
        "  \n",
        "  #creating features triplets and their labels\n",
        "  labels = construct_labels(train_file)\n",
        "  if build_tensor_bol:\n",
        "    print('Creating train and test tensor ...')\n",
        "    train_final = build_raw_features(features_sort,train_file,train_or_test_bol[0])\n",
        "    test_final = build_raw_features(features_sort,test_file,train_or_test_bol[1])\n",
        "\n",
        "  #classification model\n",
        "  x = x_in = Input(train_final.shape[1:])\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(0.7)(x)\n",
        "  x = Dense(1152)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(288)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(72)(x)\n",
        "    \n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(18)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dense(1)(x)\n",
        "  x = Activation('sigmoid')(x)\n",
        "  model = Model(inputs=x_in, outputs=x)\n",
        "\n",
        "  #model's attributes\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  #fitting training dataset to training labels\n",
        "  model.fit(x = train_final, y = labels, epochs=7)\n",
        "\n",
        "  #making final predictions for tesing dataset\n",
        "  y_test = model.predict(test_final)\n",
        "  y_test_thresh = np.where(y_test < 0.5, 0, 1)\n",
        "\n",
        "  #output in txt format to desired location\n",
        "  np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/predictions.txt', y_test_thresh, fmt='%d')"
      ],
      "metadata": {
        "id": "5enr9fberOvw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#executing parsed pipeline with following instructions: \n",
        "##set resize_bool for resizing images to desired shape\n",
        "##set get_features_bol for computing feature tensor of all images + output to features.pckl\n",
        "##set build_tensor_bol for building training testing dataset with feature triplets\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbbxNbJUrUI_",
        "outputId": "74677eec-2946-4db9-8665-c4e62a489099"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59515/59515 [00:00<00:00, 1442688.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train and test tensor ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59515/59515 [00:09<00:00, 6100.70it/s]\n",
            "100%|██████████| 59544/59544 [00:07<00:00, 8004.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "3720/3720 [==============================] - 23s 5ms/step - loss: 0.6126 - accuracy: 0.6541\n",
            "Epoch 2/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5846 - accuracy: 0.6837\n",
            "Epoch 3/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5749 - accuracy: 0.6919\n",
            "Epoch 4/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5680 - accuracy: 0.6976\n",
            "Epoch 5/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5642 - accuracy: 0.7027\n",
            "Epoch 6/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5597 - accuracy: 0.7044\n",
            "Epoch 7/7\n",
            "3720/3720 [==============================] - 19s 5ms/step - loss: 0.5553 - accuracy: 0.7080\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "task3_hugo_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}